{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model_encoder forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model_weight' : '../ner_model_weight/model_encoder_714.h5',\n",
    "    'embed_size' : 500,\n",
    "    'max_sent_len': 20,\n",
    "    'heads':16,\n",
    "    'head_size':4,\n",
    "    'batch_size': 64,\n",
    "    'lr' : 0.001,\n",
    "    'max_sent_len': 20,\n",
    "    'epochs': 500,\n",
    "    'drops' : [0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads=params['heads']\n",
    "head_size=params['head_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../char_6.17.json', mode='r', encoding='utf-8') as f:\n",
    "    dicts = json.load(f)\n",
    "char2id = dicts['char2id']\n",
    "id2char = dicts['id2char']\n",
    "intent2id = dicts['intent2id']\n",
    "id2intent = dicts['id2intent']\n",
    "slot2id = dicts['slot2id']\n",
    "id2slot = dicts['id2slot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x,axis=-1,keepdims=True)\n",
    "    y = exp_x / sum_exp_x\n",
    "    return y\n",
    "\n",
    "def embedding(x,embed_size,embed):\n",
    "    x_one= np.zeros((len(x),embed_size))\n",
    "    x_one[range(len(x)), x] = 1\n",
    "    x_embed = np.dot(x_one, embed)\n",
    "    return x_embed\n",
    "\n",
    "def GlobalAveragePooling1D(x,step_axis=0):\n",
    "    return np.mean(x,axis=step_axis)\n",
    "\n",
    "def LayerNormalization(x,gamma,beta,step_axis = -1,epsilon=1e-3):\n",
    "    mean = np.mean(x,axis = step_axis)\n",
    "    mean = np.expand_dims(mean,axis=1)\n",
    "    variance = np.var(x,axis = step_axis)\n",
    "    variance = np.expand_dims(variance,axis=1)\n",
    "    inv = 1.0 / np.sqrt(variance + epsilon)\n",
    "#     print(np.shape(inv))\n",
    "    gamma = np.expand_dims(gamma,axis=0)\n",
    "    beta = np.expand_dims(beta,axis=0)\n",
    "    inv = gamma *inv\n",
    "    return x * inv + (beta - mean * inv)\n",
    "\n",
    "def dense(x,gamma, bias):\n",
    "#     print(np.shape(x))\n",
    "    y = np.matmul(x,gamma)\n",
    "    y = np.add(y,bias)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadAttention(x,heads,head_size,atten_q_bias,atten_q_kernel,atten_k_bias,atten_k_kernel,atten_v_bias,atten_v_kernel,atten_o_bias,atten_o_kernel):\n",
    "    q = dense(x,atten_q_kernel,atten_q_bias)\n",
    "#     print('q: ',q)\n",
    "    k = dense(x,atten_k_kernel,atten_k_bias)\n",
    "#     print('k: ',k)\n",
    "    v = dense(x,atten_v_kernel,atten_v_bias)\n",
    "#     print('v: ',v )model_encoder\n",
    "    \n",
    "    qw = np.reshape(q,(-1,heads,head_size))\n",
    "    kw = np.reshape(k,(-1,heads,head_size))\n",
    "    vw = np.reshape(v,(-1,heads,head_size))\n",
    "    print(np.shape(qw))\n",
    "    \n",
    "    a = np.einsum('jhd,khd->hjk', qw, kw)\n",
    "    a = a / head_size ** 0.5\n",
    "    A = softmax(a)\n",
    "    o = np.einsum('hjk,khd -> jhd', A, vw)\n",
    "    \n",
    "    print(np.shape(o))\n",
    "    o = np.reshape(o,(-1,heads*head_size))\n",
    "    print(np.shape(o))\n",
    "    o = dense(o,atten_o_kernel,atten_o_bias)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../ner_model_weight/model_encoder_714.h5'\n",
    "f = h5py.File(file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(file_path):\n",
    "    f = h5py.File(file_path,'r')\n",
    "    \n",
    "    embed = f['embedding']['embedding']['embeddings:0'][:]\n",
    "    \n",
    "    encoder_bias = f['encoder']['encoder']['feed']['bias:0'][:]\n",
    "    encoder_kernel = f['encoder']['encoder']['feed']['kernel:0'][:]\n",
    "    \n",
    "    encoder_normal_beta = f['encoder']['encoder']['normal']['beta:0'][:]\n",
    "    encode_normal_gamma = f['encoder']['encoder']['normal']['gamma:0'][:]\n",
    "    \n",
    "    encoder_normal_beta1 = f['encoder']['encoder']['normal1']['beta:0'][:]\n",
    "    encoder_normal_gamma1 = f['encoder']['encoder']['normal1']['gamma:0'][:]\n",
    "    \n",
    "    atten_q_bias = f['encoder']['encoder']['multi_head_attention']['q']['bias:0'][:]\n",
    "    atten_q_kernel = f['encoder']['encoder']['multi_head_attention']['q']['kernel:0'][:]\n",
    "    \n",
    "    atten_k_bias = f['encoder']['encoder']['multi_head_attention']['k']['bias:0'][:]\n",
    "    atten_k_kernel = f['encoder']['encoder']['multi_head_attention']['k']['kernel:0'][:]\n",
    "    \n",
    "    atten_v_bias = f['encoder']['encoder']['multi_head_attention']['v']['bias:0'][:]\n",
    "    atten_v_kernel = f['encoder']['encoder']['multi_head_attention']['v']['kernel:0'][:]\n",
    "    \n",
    "    atten_o_bias = f['encoder']['encoder']['multi_head_attention']['o']['bias:0'][:]\n",
    "    atten_o_kernel = f['encoder']['encoder']['multi_head_attention']['o']['kernel:0'][:]\n",
    "    \n",
    "    pre_intent_bias = f['pre_intent']['pre_intent']['bias:0'][:]\n",
    "    pre_intent_kernel = f['pre_intent']['pre_intent']['kernel:0'][:]\n",
    "    \n",
    "    pre_ner_bias = f['pre_ner']['pre_ner']['bias:0'][:]\n",
    "    pre_ner_kernel = f['pre_ner']['pre_ner']['kernel:0'][:]\n",
    "    \n",
    "    return embed,encoder_bias,encoder_kernel,encoder_normal_beta,encode_normal_gamma,encoder_normal_beta1,encoder_normal_gamma1,\\\n",
    "            atten_q_bias,atten_q_kernel,atten_k_bias,atten_k_kernel,atten_v_bias,atten_v_kernel,atten_o_bias,atten_o_kernel,\\\n",
    "            pre_intent_bias,pre_intent_kernel,pre_ner_bias,pre_ner_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2labelid(vocab, labels, max_sent_len):\n",
    "    labels = [vocab[label] for label in labels]\n",
    "    if len(labels) < max_sent_len:\n",
    "        labels += [0] * (max_sent_len - len(labels))\n",
    "    else:\n",
    "        labels = labels[:max_sent_len]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(inputs,h5file_path):\n",
    "    embed,encoder_bias,encoder_kernel,encoder_normal_beta,encode_normal_gamma,encoder_normal_beta1,encoder_normal_gamma1,\\\n",
    "            atten_q_bias,atten_q_kernel,atten_k_bias,atten_k_kernel,atten_v_bias,atten_v_kernel,atten_o_bias,atten_o_kernel,\\\n",
    "            pre_intent_bias,pre_intent_kernel,pre_ner_bias,pre_ner_kernel = get_weight(file_path)\n",
    "    \n",
    "    x = trans2labelid(char2id,inputs,params['max_sent_len'])\n",
    "    embed = embedding(x,params['embed_size'], embed)\n",
    "    state = embed\n",
    "    for i in range(3):\n",
    "        att = MultiHeadAttention(state,heads,head_size,atten_q_bias,atten_q_kernel,atten_k_bias,\\\n",
    "                                 atten_k_kernel,atten_v_bias,atten_v_kernel,atten_o_bias,atten_o_kernel)\n",
    "        att_1 = np.add(att,state)\n",
    "        l = LayerNormalization(att_1,encode_normal_gamma,encoder_normal_beta)\n",
    "        feed1 = dense(l,encoder_kernel,encoder_bias)\n",
    "        l1 = LayerNormalization(feed1,encoder_normal_gamma1,encoder_normal_beta1)\n",
    "        state = l1\n",
    "    conv = GlobalAveragePooling1D(state)\n",
    "    \n",
    "    pre_intent = dense(conv,pre_intent_kernel,pre_intent_bias)\n",
    "    pre_intent = sigmoid(pre_intent)\n",
    "    \n",
    "    pre_slot = dense(state,pre_ner_kernel,pre_ner_bias)\n",
    "    pre_slot = sigmoid(pre_slot)\n",
    "    \n",
    "    return pre_intent, pre_slot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 16, 4)\n",
      "(20, 16, 4)\n",
      "(20, 64)\n",
      "(20, 16, 4)\n",
      "(20, 16, 4)\n",
      "(20, 64)\n",
      "(20, 16, 4)\n",
      "(20, 16, 4)\n",
      "(20, 64)\n"
     ]
    }
   ],
   "source": [
    "inputs = '打开空调'\n",
    "np_pre_intent, np_pre_slot  = test(inputs,file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.95904586e-09, 2.04386779e-05, 1.66595914e-04, 9.81184326e-06,\n",
       "       1.24571212e-04, 5.37920218e-06, 2.57746185e-04, 1.79327330e-04,\n",
       "       6.70664053e-06, 4.14397897e-06, 1.61926877e-05, 6.62351278e-09,\n",
       "       6.09047303e-05, 6.30200950e-08, 5.87803221e-05, 1.04552780e-05,\n",
       "       5.56724286e-05, 1.74767173e-06, 5.83089042e-06, 1.70645293e-05,\n",
       "       1.26748242e-06, 2.43458928e-06, 1.42205218e-04, 1.67237164e-05,\n",
       "       4.44439417e-06, 1.15679629e-04, 1.75295308e-06, 1.68209113e-05,\n",
       "       9.88302989e-05, 3.50426021e-06, 4.88769135e-05, 3.19596877e-05,\n",
       "       1.85471583e-06, 1.43897584e-09, 2.72267536e-08, 3.83518695e-05,\n",
       "       9.27672908e-06, 1.60597614e-06, 1.57129852e-05, 9.12048096e-06,\n",
       "       1.67779951e-07, 4.94079045e-05, 7.63037671e-07, 4.46608465e-05,\n",
       "       9.90124232e-08, 5.94714345e-06, 3.40175920e-01, 1.54744429e-04,\n",
       "       8.20822876e-06, 1.23600913e-04, 1.37792943e-06, 3.70436733e-06,\n",
       "       1.23600317e-07, 4.13216434e-06, 9.34163704e-06])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_pre_intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate, Dropout,LayerNormalization, Dense, add\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            heads,\n",
    "            head_size,\n",
    "            out_dim=None,\n",
    "            use_bias=True,\n",
    "#             max_value = 1,\n",
    "#             min_value = -1l1\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.heads = heads\n",
    "        self.head_size = head_size\n",
    "        self.out_dim = out_dim \n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "        self.q_dense = tf.keras.layers.Dense(\n",
    "            units=self.head_size * self.heads,\n",
    "            use_bias=self.use_bias,\n",
    "            kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            bias_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            name = 'q'\n",
    "            \n",
    "        )\n",
    "        self.k_dense = tf.keras.layers.Dense(\n",
    "            units=self.head_size * self.heads,\n",
    "            use_bias=self.use_bias,\n",
    "            kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            bias_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            name = 'k'\n",
    "        )\n",
    "        self.v_dense = tf.keras.layers.Dense(\n",
    "            units=self.head_size * self.heads,\n",
    "            use_bias=self.use_bias,\n",
    "            kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            bias_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            name = 'v'\n",
    "        )\n",
    "        self.o_dense = tf.keras.layers.Dense( \n",
    "            units=self.out_dim,\n",
    "            use_bias=self.use_bias,\n",
    "            kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            bias_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            name = 'o'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = inputs\n",
    "        k = inputs\n",
    "        v = inputs\n",
    "        # 线性变化\n",
    "        qw = self.q_dense(q)\n",
    "        kw = self.k_dense(k)\n",
    "        vw = self.v_dense(v)\n",
    "        # 形状变换\n",
    "        qw = tf.reshape(qw, (-1, tf.shape(q)[1], self.heads, self.head_size))\n",
    "        kw = tf.reshape(kw, (-1, tf.shape(q)[1], self.heads, self.head_size))\n",
    "        vw = tf.reshape(vw, (-1, tf.shape(q)[1], self.heads, self.head_size))\n",
    "        # attention\n",
    "        qkv_inputs = [qw, kw, vw]\n",
    "        o = self.pay_attention_to(qkv_inputs)\n",
    "        o = tf.reshape(o, (-1, tf.shape(o)[1], self.head_size * self.heads))\n",
    "        o = self.o_dense(o)\n",
    "        return o\n",
    "\n",
    "    def pay_attention_to(self, inputs):\n",
    "        (qw, kw, vw) = inputs[:3]\n",
    "        a = tf.einsum('bjhd,bkhd->bhjk', qw, kw)\n",
    "        a = a / self.head_size ** 0.5\n",
    "        A = tf.nn.softmax(a)\n",
    "        o = tf.einsum('bhjk,bkhd -> bjhd', A, vw)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.layer_count = layer_count\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.MultiHeadAttention =  MultiHeadAttention(heads=16,head_size=4,out_dim=32)\n",
    "        self.dropout_1 = Dropout(0.1)\n",
    "        self.l1 =  LayerNormalization(name='normal')\n",
    "        self.feed1 = Dense(32,name='feed')\n",
    "        self.dropout1 = Dropout(0.1)\n",
    "        self.l_1 =  LayerNormalization(name='normal1')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        state = inputs\n",
    "        for _ in range(self.layer_count):\n",
    "            att1 = self.MultiHeadAttention(state)\n",
    "            att_1 = add([att1,state])\n",
    "            dropout1  = self.dropout_1(att_1)\n",
    "            l1 = self.l1(dropout1)\n",
    "            feed1 =self.feed1(l1)\n",
    "            dropout_1  = self.dropout1(feed1)\n",
    "            l_1 = self.l_1(dropout_1)\n",
    "            state = l_1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['intent_num'] = len(intent2id)\n",
    "params['slot_num'] = len(slot2id)\n",
    "params['id2intent'] = id2intent\n",
    "params['id2slot'] = id2slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 32)       16000       Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Encoder)               (None, 20, 32)       9600        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pre_intent (Dense)              (None, 55)           1815        global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "pre_ner (Dense)                 (None, 20, 36)       1188        encoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 28,603\n",
      "Trainable params: 28,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "text_inputs = tf.keras.layers.Input(shape=(20,),name='Input')\n",
    "embed = tf.keras.layers.Embedding(500,32)(text_inputs)\n",
    "\n",
    "l_1 = Encoder(layer_count=3)(embed)\n",
    "\n",
    "conv = tf.keras.layers.GlobalAveragePooling1D()(l_1)\n",
    "pre_intent = tf.keras.layers.Dense(params['intent_num'],activation='sigmoid',name = 'pre_intent',kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            bias_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0))(conv)\n",
    "pre_slot = tf.keras.layers.Dense(params['slot_num'],activation='sigmoid',name = 'pre_ner',kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0),\n",
    "            bias_constraint = tf.keras.constraints.MinMaxNorm(min_value=-1.0))(l_1)\n",
    "model = tf.keras.Model(text_inputs,[pre_intent,pre_slot])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../ner_model_weight/model_encoder_714.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trans2labelid(char2id,inputs,params['max_sent_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_intent,pre_slot = model.predict([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9590465e-09, 2.0438689e-05, 1.6659604e-04, 9.8118408e-06,\n",
       "        1.2457115e-04, 5.3791969e-06, 2.5774655e-04, 1.7932746e-04,\n",
       "        6.7066326e-06, 4.1439694e-06, 1.6192693e-05, 6.6235017e-09,\n",
       "        6.0904629e-05, 6.3020160e-08, 5.8780417e-05, 1.0455287e-05,\n",
       "        5.5672532e-05, 1.7476705e-06, 5.8308751e-06, 1.7064507e-05,\n",
       "        1.2674833e-06, 2.4345902e-06, 1.4220510e-04, 1.6723672e-05,\n",
       "        4.4443959e-06, 1.1567961e-04, 1.7529503e-06, 1.6820906e-05,\n",
       "        9.8830460e-05, 3.5042683e-06, 4.8876893e-05, 3.1959706e-05,\n",
       "        1.8547166e-06, 1.4389775e-09, 2.7226763e-08, 3.8351842e-05,\n",
       "        9.2767259e-06, 1.6059797e-06, 1.5712985e-05, 9.1204747e-06,\n",
       "        1.6777959e-07, 4.9407943e-05, 7.6303849e-07, 4.4660752e-05,\n",
       "        9.9012532e-08, 5.9471431e-06, 3.4017593e-01, 1.5474478e-04,\n",
       "        8.2082279e-06, 1.2360090e-04, 1.3779290e-06, 3.7043731e-06,\n",
       "        1.2360017e-07, 4.1321618e-06, 9.3416484e-06]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
